{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import Imputer, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import featuretools as ft\n",
    "from featuretools import primitives as prims\n",
    "from featuretools.selection.variance_selection import (\n",
    "    select_high_variance_features,\n",
    "    select_percent_null)\n",
    "import os\n",
    "from ml import (bin_labels,\n",
    "                TimeSeriesSplitByDate,\n",
    "                fit_and_score)\n",
    "from utils import (build_baseline_features,\n",
    "                   load_entityset,\n",
    "                   get_feature_importances,\n",
    "                   plot_confusion_matrix)\n",
    "from IPython.display import display\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Featuretools to analyze medals won at the Olympic Games\n",
    "\n",
    "In this notebook, we will examine a dataset containing all the medals won by each athlete at each Summer Olympic Games. Our goal will be to see what quantities are important in predicting the future number of medals won.\n",
    "\n",
    "To do this, we will build a machine-learning-based predictive model that is trained on historical data, and used to predict the number of medals won by each country in the next Olympics. we'll build this model using high-level transformations of the data-- called features-- that are automatically generated from Featuretools. [Featuretools](https://featuretools.com) is a Python library for [automated feature engineering]().\n",
    "\n",
    "This notebook will serve as an introduction to Featuretools, but does not explain each API call in depth. Please see the [documentation](https://docs.featuretools.com) for a more thorough usage guide.\n",
    "\n",
    "## Disclaimer on suitability of the dataset and problem\n",
    "\n",
    "As we'll see, basic machine learning allows us to predict the particular quantities we care about here extremely well. The advanced high-level features provided by basic Featuretools API calls do not produce that much improvement over our simple baseline model. However, what they do provide are much more interpretable predictive factors, so that we can examine which were factors were most important over time. In the other notebooks in this repo, we will see how to use the more advanced components of Featuretools to both increase our scores and, more importantly, to see how additional types of features affect the predictive power of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DATA_FOLDER = os.path.expanduser(\"~/olympic_games_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load in data\n",
    "\n",
    "Check out LoadEntityset.ipynb for a walkthrough of how to set up a dataset for use by Featuretools.\n",
    "\n",
    "`EntitySet` is the in-memory data structure Featuretools uses to build and calculate features. It essentially consists of a dictionary of [Pandas DataFrames](https://pandas.pydata.org/pandas-docs/stable/dsintro.html) with associated metadata on how they are linked, what semantic types they contain, and how they vary in time. For an in-depth guide on. For more details and tutorials, please check out the [documentation](https://docs.featuretools.com/loading_data/using_entitysets.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Entityset: Olympic Games\n",
       "  Entities:\n",
       "    disciplines (shape = [67, 3])\n",
       "    countries (shape = [220, 8])\n",
       "    olympic_games (shape = [27, 3])\n",
       "    medals_won (shape = [11532, 6])\n",
       "    sports (shape = [43, 2])\n",
       "    ...And 3 more\n",
       "  Relationships:\n",
       "    medaling_athletes.Athlete -> athletes.Athlete\n",
       "    medals_won.Country Olympic ID -> countries_at_olympic_games.Country Olympic ID\n",
       "    countries_at_olympic_games.Olympic Games ID -> olympic_games.Olympic Games ID\n",
       "    medals_won.Discipline -> disciplines.Discipline\n",
       "    disciplines.Sport -> sports.Sport\n",
       "    ...and 2 more"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = load_entityset()\n",
    "es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load labels and cutoff times\n",
    "\n",
    "Machine learning (in its [supervised](http://scikit-learn.org/stable/supervised_learning.html) flavor) is all about predicting a set of labels, given a set of training data. This is canonically represented by a *feature matrix*, whose rows are each numeric vectors of fixed length. Each row is associated with a single value, called a label. The machine learning algorithm is tasked with building a model that is good at predicting the label given the row vector.\n",
    "\n",
    "What makes *predictive modeling* more interesting is that the labels actually represent future quantities which we want to predict using historical data. This means that the time associated with the label is extremely important, because we need to know what historical data is valid (we don't want to use data that was recorded *after* the label was known).\n",
    "\n",
    "In this particular case, our goal is to predict the number of medals won by each country in each subsequent Olympic Games, so our labels will be uniquely defined by a particular country and date of particular Olympic Games in which it competed.\n",
    "\n",
    "Check out GeneratingLabels.ipynb for a walkthrough of how I generated these labels. Shameless plug: [Feature Labs](https://www.featurelabs.com/), the company that I work for and that maintains Featuretools, sells a platform that, among other things, makes this label generation process extremely easy. This process is known as [Prediction Engineering](https://www.featurelabs.com/resources/why.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Medals</th>\n",
       "      <th>Olympics Date</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>1896-06-29 00:00:00</td>\n",
       "      <td>AUS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5</td>\n",
       "      <td>1896-06-29 00:00:00</td>\n",
       "      <td>AUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1896-06-29 00:00:00</td>\n",
       "      <td>DEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1896-06-29 00:00:00</td>\n",
       "      <td>FRA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>1896-06-29 00:00:00</td>\n",
       "      <td>GBR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Number of Medals       Olympics Date Country\n",
       "8                  2 1896-06-29 00:00:00     AUS\n",
       "9                  5 1896-06-29 00:00:00     AUT\n",
       "5                  6 1896-06-29 00:00:00     DEN\n",
       "10                11 1896-06-29 00:00:00     FRA\n",
       "4                  7 1896-06-29 00:00:00     GBR"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_file = os.path.join(ROOT_DATA_FOLDER, \"num_medals_by_country_labels.csv\")\n",
    "label_df = pd.read_csv(label_file,\n",
    "                       parse_dates=['Olympics Date'],\n",
    "                       encoding='utf-8',\n",
    "                       usecols=['Number of Medals', 'Olympics Date', 'Country'])\n",
    "# Sort by the date of the Olympics, and by the country (to maintain a consistent ordering)\n",
    "label_df.sort_values(['Olympics Date', 'Country'], inplace=True)\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Features Using Deep Feature Synthesis\n",
    "\n",
    "\n",
    "Featuretools automatically extracts high-level, interpretable features from EntitySets. This means we can exhaustively create many features, and use these to train our machine learning predictive model.\n",
    "\n",
    "Moreover, it makes sure to calculate these features only using data on or before specified *cutoff times*. In this case, these cutoff times will be dates immediately prior to each Olympic Games. They are specified in the label file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just want the Country and time to compute features (we're removing the label column here)\n",
    "cutoff_times = label_df[['Country', 'Olympics Date']]\n",
    "# Code is the index of the \"countries\" entity in the entityset (short for Country Code)\n",
    "cutoff_times = cutoff_times.rename(columns={'Country': 'Code'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify the particular primitives we want Featuretools to use to construct features. Featuretools will walk through our EntitySet and apply these primitives recursively using the [Deep Feature Synthesis](https://docs.featuretools.com/automated_feature_engineering/afe.html) algorithm. These primitives are basic functions that take a particular data type as input (such as a Numeric or Categorical), and output a single value per *instance* of an entity.\n",
    "\n",
    "Transform primitives take each instance (row) of an entity and apply a function to each row.\n",
    "\n",
    "Aggregation primitives take 2 entities, where each row in the *parent* entity is connected to many rows in the *child*, and apply a function to all the rows in the *child* connected to a single *parent* row.\n",
    "\n",
    "\n",
    "These primitives are defined in more detail [here](https://docs.featuretools.com/automated_feature_engineering/primitives.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_primitives = [\n",
    "    prims.Sum, prims.Std, prims.Max, prims.Min, prims.Mean, prims.Count,\n",
    "    prims.PercentTrue, prims.NUnique, prims.Mode, prims.Trend, prims.Skew\n",
    "]\n",
    "trans_primitives = [\n",
    "    prims.Percentile\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison to a Baseline\n",
    "\n",
    "We want to make sure we use the features we use for our simple baseline model for a fair comparison to our more interpretable Featuretools model. To do this, we define these features manually using Featuretools, and add them in as *seed features* to DFS. This will not only include these features directly, but will also build higher-level features on top of them.\n",
    "\n",
    "For a more thorough walkthrough of the baseline, check out BaselineSolutions.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include our baseline predictors as seed features\n",
    "num_medals_each_olympics, mean_num_medals = build_baseline_features(es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run DFS\n",
    "\n",
    "A note on the parameter settings:\n",
    "  * `target_entity` defines the entity containing unique rows (called *instances*) for each training example. \n",
    "  * `trans_primitives` is a list of *transform* primitive classes.\n",
    "  * `agg_primitives` is a list of *aggregation* primitive classes.\n",
    "  * `max_depth` defines how *deep* we build features on top of one another ([explanation](https://docs.featuretools.com/automated_feature_engineering/afe.html#creating-deep-features))\n",
    "  * `cutoff_times` is a Pandas DataFrame with a column for defining each instance we want to compute features for, and its associated cutoff time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building features: 2818it [00:03, 552.46it/s] \n",
      "Progress:  78%|███████▊  | 21/27 [13:17<06:05, 60.96s/cutoff time]"
     ]
    }
   ],
   "source": [
    "feature_matrix, features = ft.dfs(\n",
    "    entityset=es,\n",
    "    target_entity=\"countries\",\n",
    "    trans_primitives=trans_primitives,\n",
    "    agg_primitives=agg_primitives,\n",
    "    max_depth=4,\n",
    "    seed_features=num_medals_each_olympics + [mean_num_medals],\n",
    "    cutoff_time=cutoff_times,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove features with zero variance or almost all null\n",
    "\n",
    "Featuretools contains a couple handy functions for removing features with mostly null values, or with very little variance. Here, we remove the features that have 90% or greater null values, and whose variance (actually [coefficient of variation](https://en.wikipedia.org/wiki/Coefficient_of_variation)) is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix, features = select_percent_null(feature_matrix, features, max_null_percent=90)\n",
    "\n",
    "print \"%d low percent null features selected\" % len(features)\n",
    "\n",
    "feature_matrix, features = select_high_variance_features(feature_matrix, features, cv_threshold=0)\n",
    "\n",
    "print \"%d high variance features selected\" % len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-hot-encode categorical features\n",
    "\n",
    "Machine learning algorithms require all numeric values as input, but Featuretools by default produces some categorical values. One simple way to transform these values into numbers is to create several binary features that encode whether the feature is equal to each particular category (by default capped at the top 10 most common categories). For more information check out this [docs page](https://docs.featuretools.com/guides/tuning_dfs.html#encoding-categorical-features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_matrix_encoded, features_encoded = ft.encode_features(feature_matrix, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binarize and bin labels\n",
    "\n",
    "We'll actually create 3 slightly different models:\n",
    "\n",
    "  1. Regression. Predicting the actual number of medals per country\n",
    "  2. Binary Classification. Predicting whether the number of medals is greater than 10.\n",
    "  3. Binned Classification. Predicting which bin the medals falls into (i.e. between 0 and 2, 2 and 6, 6 and 10, 10 and 50, or greater than 50)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = label_df['Olympics Date']\n",
    "labels = label_df['Number of Medals']\n",
    "binned_labels, bins = bin_labels(labels, [2, 6, 10, 50])\n",
    "binary_labels = (labels >= 10).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_labels.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate out the baseline features\n",
    "\n",
    "These are for the slightly more complex, machine-learning-based \"Baseline 2\" from BaselineSolutions.ipynb. Since the other baseline is not machine-learning-based, refer to that notebook for scores and analysis (they are significantly worse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "just_baseline = feature_matrix_encoded[[\n",
    "    f.get_name() for f in num_medals_each_olympics\n",
    "    if f.get_name() in feature_matrix_encoded\n",
    "]]\n",
    "just_baseline.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build machine learning models and generate scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create scikit-learn estimators\n",
    "\n",
    "We'll use stock machine learning algorithms from scikit-learn, as well as an imputer that replaces missing values with the mean over that feature, and a scaler that makes sure each feature's values vary from 0 to 1. The `RobustScaler` class is more sensitive to outliers than the `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_preprocessing = [(\"imputer\",\n",
    "                           Imputer(missing_values='NaN', strategy=\"mean\", axis=0)),\n",
    "                          (\"scaler\", RobustScaler(with_centering=True))]\n",
    "\n",
    "regression_models = {\n",
    "        'rf_regression': RandomForestRegressor(n_estimators=100, n_jobs=-1),\n",
    "}\n",
    "classification_models = {\n",
    "        'rf_clf': RandomForestClassifier(n_estimators=100, n_jobs=-1),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-based cross-validation\n",
    "\n",
    "The way we'll do cross-validation is to separate training sets for each Olympics since 1960, using all historical data in the past for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = TimeSeriesSplitByDate(dates=dates, earliest_date=pd.Timestamp('1/1/1960'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = feature_matrix_encoded.values\n",
    "y = labels.values\n",
    "y_binary = binary_labels.values\n",
    "y_binned = binned_labels.values\n",
    "X_baseline = just_baseline.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "`fit_and_score` takes in a feature matrix, labels, splitter, and the machine learning pipeline, builds models for each split, and scores each one using various metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in regression_models.items():\n",
    "    reg_pipeline = Pipeline(pipeline_preprocessing + [(name, model)])\n",
    "    scores = fit_and_score(X, y, splitter, reg_pipeline, _type='regression')\n",
    "    baseline_scores = fit_and_score(X_baseline, y, splitter, reg_pipeline, _type='regression')\n",
    "    print \"Regression model %s\" % name\n",
    "    r2_scores = scores['r2']\n",
    "    mse_scores = scores['mse']\n",
    "    print \"  R2 mean score:  %.2f +/- %.2f\" % (scores['r2'].mean(),\n",
    "                                               scores['r2'].std())\n",
    "    print \"  Baseline R2 mean score:  %.2f +/- %.2f\" % (baseline_scores['r2'].mean(),\n",
    "                                                        baseline_scores['r2'].std())\n",
    "    print \"  MSE mean score:  %.2f +/- %.2f\" % (scores['mse'].mean(),\n",
    "                                                scores['mse'].std())\n",
    "    print \"  Baseline MSE mean score:  %.2f +/- %.2f\" % (baseline_scores['mse'].mean(),\n",
    "                                                         baseline_scores['mse'].std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, model in classification_models.items():\n",
    "    pipeline = Pipeline(pipeline_preprocessing + [(name, model)])\n",
    "    binary_scores = fit_and_score(X, y_binary, splitter, pipeline, _type='classification')\n",
    "    baseline_binary_scores = fit_and_score(X_baseline, y_binary, splitter, pipeline, _type='classification')\n",
    "    binned_scores = fit_and_score(X, y_binned, splitter, pipeline, _type='classification')\n",
    "    baseline_binned_scores = fit_and_score(X_baseline, y_binned, splitter, pipeline, _type='classification')\n",
    "    print \"Classification model %s\" % name\n",
    "    print \"  AUC mean score:  %.2f +/- %.2f\" % (binary_scores['roc_auc'].mean(),\n",
    "                                                binary_scores['roc_auc'].std())\n",
    "    print \"  Baseline AUC mean score:  %.2f +/- %.2f\" % (baseline_binary_scores['roc_auc'].mean(),\n",
    "                                                         baseline_binary_scores['roc_auc'].std())\n",
    "    print \"  F1 mean score:  %.2f +/- %.2f\" % (binary_scores['f1'].mean(),\n",
    "                                               binary_scores['f1'].std())\n",
    "    print \"  Baseline F1 mean score:  %.2f +/- %.2f\" % (baseline_binary_scores['f1'].mean(),\n",
    "                                                        baseline_binary_scores['f1'].std())\n",
    "    print \"  Binned F1 (micro averaged) mean score:  %.2f +/- %.2f\" % (binned_scores['f1_micro'].mean(),\n",
    "                                                                       binned_scores['f1_micro'].std())\n",
    "    print \"  Baseline Binned F1 (micro averaged) mean score:  %.2f +/- %.2f\" % (baseline_binned_scores['f1_micro'].mean(),\n",
    "                                                                                baseline_binned_scores['f1_micro'].std())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the regression scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.set_index('Olympics Year')['r2'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.set_index('Olympics Year')['mse'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the binned scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binned_scores.set_index('Olympics Year')['mse'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breakdown of scores\n",
    "\n",
    "Our scores on the whole are very good, yet break down for particular years. We'll examine why this is further down in this notebook.\n",
    "\n",
    "\n",
    "## Do we beat the baseline solutions?\n",
    "# TODO: redo this\n",
    "We definitely beat the baseline 1 in all 3 types of problems. Recall that baseline 1 simply used the mean number of medals each country had won in prior Olympics.\n",
    "\n",
    "We beat baseline 2 in the classification task, and more so in the binary case than the binned case.\n",
    "\n",
    "We do about the same in the regression case, with the same weird bump about a third of the way through (which happens to be the 1976 and 1980 Olympic Games. The very simple solutions seem to work well for regression. However, our complex feature engineering provides us a way to examine other hidden factors that are predictive of the outcome.\n",
    "\n",
    "Since this is a relatively easy machine learning problem, many different features will be predictive of the outcome, and it's more interesting to examine which of these these factors were predictive, and maybe take a stab at why they were, rather than trying to blindly increase predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix\n",
    "# TODO: show this for various years\n",
    "As a first step in our analysis, let's plot the confusion matrix for the last split (using code taken from Scikit-Learn [here](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can plot the confusion matrix for our binary classification problem\n",
    "train, test = splitter.split(X, y_binary)[-8]\n",
    "pipeline.fit(X[train], y_binary[train])\n",
    "y_pred = pipeline.predict(X[test])\n",
    "cm = confusion_matrix(y_binary[test], y_pred)\n",
    "plot_confusion_matrix(cm, ['Won < 10 Medals', 'Won >= 10 Medals'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we do better at predicting the low-medaling countries than the high-medaling countries (our false positive rate (3/28) is a bit lower than our false negative rate (7/85))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine feature importances\n",
    "\n",
    "This utility function grabs the Scikit-Learn Random Forest's built in feature importances, sorts them, and groups them by time split. These refer to how important each feature was in predicting the label.\n",
    "\n",
    "We'll print out the top 10 most predictive features for each Olympics since 1960."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imps_over_time = get_feature_importances(pipeline,\n",
    "                                                 feature_matrix_encoded,\n",
    "                                                 binary_labels,\n",
    "                                                 splitter,\n",
    "                                                 n=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for date, fi in sorted(feature_imps_over_time.items(), key=lambda x: x[0]):\n",
    "    print \"Test year: %s\" % date.year\n",
    "    print \"Top 10 feats:\"\n",
    "    for f in fi['Feature'][:10]:\n",
    "        print \"   \", f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that features built on Baseline 1 (\"mean_num_medals\") are pretty high up in the list, but we do some interesting transformations on them. In particular, the Percentile feature is important across the board.\n",
    "\n",
    "We will examine particular features in more depth later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Feature Selection\n",
    "\n",
    "One interesting workflow that Featuretools allows is the ability to select the most important features from an exhaustive list based on which were actually useful in predicting the labels.\n",
    "\n",
    "To do this, let's rerun the computation using just the top 400 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top400 = feature_imps_over_time[pd.Timestamp('6/29/2012')]['Feature'].tolist()\n",
    "important_feature_matrix = feature_matrix[top400]\n",
    "\n",
    "for name, model in classification_models.items():\n",
    "    pipeline = Pipeline(pipeline_preprocessing + [(name, model)])\n",
    "    scores = fit_and_score(important_feature_matrix.values, y_binary, splitter, pipeline, _type='classification')\n",
    "    print \"Classification model %s\" % name\n",
    "    print \"  AUC mean score:  %.2f +/- %.2f\" % (scores['roc_auc'].mean(),\n",
    "                                                scores['roc_auc'].std())\n",
    "    print \"  F1 mean score:  %.2f +/- %.2f\" % (scores['f1'].mean(),\n",
    "                                               scores['f1'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison to full feature set\n",
    "\n",
    "We do slightly better than both our original model and our baseline now. However, in absolute terms our scores did not change very much because they were so high to begin with. What's more interesting is that we now have a small set of highly explainable features. We can reduce this feature set even more without a significant drop in accuracy. Here it is with 50 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_feature_matrix = feature_matrix[top400[:50]]\n",
    "for name, model in classification_models.items():\n",
    "    pipeline = Pipeline(pipeline_preprocessing + [(name, model)])\n",
    "    scores = fit_and_score(important_feature_matrix.values, y_binary, splitter, pipeline, _type='classification')\n",
    "    print \"Classification model %s\" % name\n",
    "    print \"  AUC mean score:  %.2f +/- %.2f\" % (scores['roc_auc'].mean(),\n",
    "                                                scores['roc_auc'].std())\n",
    "    print \"  F1 mean score:  %.2f +/- %.2f\" % (scores['f1'].mean(),\n",
    "                                               scores['f1'].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smaller set of important features\n",
    "\n",
    "Now let's take a closer look at the important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imps_over_time = get_feature_importances(pipeline,\n",
    "                                                 important_feature_matrix,\n",
    "                                                 binary_labels,\n",
    "                                                 splitter,\n",
    "                                                 n=50)\n",
    "\n",
    "for date, fi in sorted(feature_imps_over_time.items(), key=lambda x: x[0]):\n",
    "    print \"Test year: %s\" % date.year\n",
    "    print \"Top 10 feats:\"\n",
    "    for f in fi['Feature'][:50]:\n",
    "        print \"   \", f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see something interesting. Starting in 2008, we see the Percentile of the number of medals won in Olympics 19 as extremely important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "olympics = es['olympic_games'].df\n",
    "olympics[olympics['Olympic Games ID'] == 11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Olympics 19 corresponds to the LA Olympics in 1984. Why is this particular Olympics so important?\n",
    "\n",
    "Check out our performance over time. The LA Olympics is the lowest score we see for AUC, and second lowest for F1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scores[['f1', 'Olympics Year']].set_index('Olympics Year').plot(kind='bar')\n",
    "scores.set_index('Olympics Year').plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Olympics in the 1980s were marred by controversies. Summarizing the\n",
    "[Wikipedia article](https://en.wikipedia.org/wiki/List_of_Olympic_Games_scandals_and_controversies):\n",
    "1. Rule changes\n",
    "   - This is the period that the IOC gradually started allowing professional\n",
    "   athletes\n",
    "2. Geopolitics\n",
    "   - Widespread boycotts due to the Cold War\n",
    "   - Montreal 1976: 22 African nations boycotted (to protest New Zealand's\n",
    "   rugby team's tour of South Africa during Apartheid). China and Taiwan both\n",
    "   boycotted. Fewest number of countries (92) participating since 1960.\n",
    "   - Moscow 1980: US boycotts because of Soviet invasion of Afghanistan.\n",
    "      Even fewer number of countries (80) than 1976\n",
    "   - Los Angeles 1984: Soviet Union and 15 Eastern Bloc nations boycott\n",
    "   because of \"safety concerns\". (However, a record 140 National Olympic\n",
    "   Committees took part)\n",
    "      Even fewer number of countries (80) than 1976\n",
    "3. Performance-enhancing Drugs\n",
    "   - late 1970s and especially 1980s ushered in the era of steroids\n",
    "   - quote from 1989 Australian study \"There is hardly a medal winner at the\n",
    "   Moscow Games, certainly not a gold medal winner, who is not on one sort of\n",
    "   drug or another: usually several kinds. The Moscow Games might as well have\n",
    "   been called the Chemists' Games.\" See the wiki\n",
    "   [page](https://en.wikipedia.org/wiki/Olympic_Games#Use_of_performance-enhancing_drugs), citation 183.\n",
    "4. Gender discrimination\n",
    "   - Many countries didn't send women until the late 1990s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a more in-depth analysis, check out AdvancedFeaturetools.ipynb and LinkingDatasets.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's make a prediction about the 2016 olympics\n",
    "\n",
    "We've trained models to predict the highest medaling countries in past Olympic Games, so we might as well use those models to predict who will receive the most medals in the next Olympics. This dataset came out before 2016, so we can compare to what actually happened. Unfortunately, the way our model is set up we would have to add in the 2016 data to make predictions about 2020 (I'll leave that up to you to try out yourself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on all data\n",
    "reg_pipeline.fit(important_feature_matrix, y)\n",
    "\n",
    "# Get the top feature objects to compute\n",
    "important_features = {f.get_name(): f for f in features_encoded\n",
    "                      if f.get_name() in important_feature_matrix.columns}.values()\n",
    "# Compute feature matrix on all data (no cutoff time)\n",
    "# Leave cutoff_date blank to use the current time\n",
    "df = es['countries_at_olympic_games'].df\n",
    "countries_2012 = df.loc[df['Year'] == pd.Timestamp('2012-06-30'), 'Country']\n",
    "latest_fm = ft.calculate_feature_matrix(features=important_features,\n",
    "                                        instance_ids=countries_2012,\n",
    "                                        cutoff_time=None,\n",
    "                                        verbose=True)\n",
    "# Make sure order is correct\n",
    "latest_fm = latest_fm[important_feature_matrix.columns]\n",
    "\n",
    "predictions = reg_pipeline.predict(latest_fm)\n",
    "predictions_by_country = pd.Series(predictions, index=latest_fm.index)\n",
    "predictions_by_country.sort_values(ascending=False, inplace=True)\n",
    "predictions_by_country.head(20).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual medal count is available [here](https://en.wikipedia.org/wiki/2016_Summer_Olympics_medal_table).\n",
    "I took the first 20 to plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_2016 = pd.DataFrame.from_records([('USA', 121),\n",
    "('CHN', 70) ,\n",
    "('GBR', 67) ,\n",
    "('RUS', 56) ,\n",
    "('GER', 42) ,\n",
    "('FRA', 42) ,\n",
    "('JPN', 41) ,\n",
    "('AUS', 29) ,\n",
    "('ITA', 28) ,\n",
    "('CAN', 22) ,\n",
    "('KOR', 21) ,\n",
    "('NED', 19) ,\n",
    "('BRA', 19) ,\n",
    "('NZL', 18) ,\n",
    "('KAZ', 18) ,\n",
    "('AZE', 18) ,\n",
    "('ESP', 17) ,\n",
    "('HUN', 15) ,\n",
    "('DEN', 15) ,\n",
    "('KEN', 1)], columns=['Country', 'Medals']).set_index(['Country'])['Medals']\n",
    "\n",
    "actual_2016.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Looks like we overestimated the top performers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = es['countries_at_olympic_games'].df\n",
    "athletes = es['athletes_at_olympic_games'].df\n",
    "medals = es['medals_won'].df\n",
    "coid = countries[countries['Country']== 'USA']['Country Olympic ID'].iloc[-1]\n",
    "athletes = athletes[athletes['Country Olympic ID'] == 1094]\n",
    "medals = medals.merge(athletes, on='Athlete Olympic ID')\n",
    "medals[medals['Medal'] == 'Gold'].drop_duplicates(subset=['Year_x', 'Discipline', 'Event', 'Medal']).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More wildness\n",
    "What happens if we include all historical countries that ever competed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave instance_ids blank to use all\n",
    "latest_fm = ft.calculate_feature_matrix(features=important_features,\n",
    "                                        cutoff_time=None,\n",
    "                                        verbose=True)\n",
    "# Make sure order is correct\n",
    "latest_fm = latest_fm[important_feature_matrix.columns]\n",
    "\n",
    "predictions = reg_pipeline.predict(latest_fm)\n",
    "predictions_by_country = pd.Series(predictions, index=latest_fm.index)\n",
    "predictions_by_country.sort_values(ascending=False, inplace=True)\n",
    "predictions_by_country.head(20).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model thinks that if the former Soviet Union were alive today it would win it all!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
